# EditalShield: Banco de Dados + Dados SintÃ©ticos

## 1. Schema de Banco de Dados (PostgreSQL)

### Arquivo: `database/schema.sql`

```sql
-- ============================================================================
-- EditalShield: Schema de BD para FundaÃ§Ã£o MatemÃ¡tica
-- ============================================================================

-- Tabela: Editais (referÃªncia)
CREATE TABLE editals (
    edital_id SERIAL PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    agency VARCHAR(100) NOT NULL,  -- FINEP, FAPESP, CNPq, SEBRAE, etc.
    min_value DECIMAL(15,2),
    max_value DECIMAL(15,2),
    execution_months INT,
    approval_rate_historical FLOAT,  -- 0.0 a 1.0
    eligible_sectors TEXT[],  -- array de setores
    eligible_stages TEXT[],  -- pre-seed, seed, growth
    technical_detail_level VARCHAR(20),  -- low, medium, high
    evaluation_type VARCHAR(50),  -- comite_publico, banca_fechada
    full_text TEXT,
    criteria_json JSONB,  -- {inovacao: 30, viabilidade: 25, ...}
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela: Memoriais (originais)
CREATE TABLE memorials (
    memorial_id SERIAL PRIMARY KEY,
    edital_id INT REFERENCES editals(edital_id),
    sector VARCHAR(100),
    technology_type VARCHAR(100),  -- software, hardware, service, hybrid
    stage VARCHAR(50),  -- pre-seed, seed, growth
    result VARCHAR(50),  -- approved, rejected, under_review
    original_text TEXT NOT NULL,
    num_words INT,
    num_paragraphs INT,
    is_synthetic BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela: ParÃ¡grafos Anotados (ground truth)
CREATE TABLE paragraphs_annotated (
    paragraph_id SERIAL PRIMARY KEY,
    memorial_id INT REFERENCES memorials(memorial_id),
    paragraph_index INT,  -- ordem no memorial
    original_text TEXT NOT NULL,
    section_type VARCHAR(50),  -- technical, market, team, admin
    has_exposure BOOLEAN NOT NULL,  -- 0 ou 1 (ground truth)
    exposure_types TEXT[],  -- array de tipos: [algoritmo, parametros, dataset, contatos, metricas]
    entropy_value FLOAT,
    entropy_normalized FLOAT,
    num_sensitive_patterns INT,
    edital_type VARCHAR(20),  -- public, confidential
    rater_1_label BOOLEAN,  -- anotador 1
    rater_2_label BOOLEAN,  -- anotador 2
    rater_3_label BOOLEAN,  -- anotador 3
    inter_rater_agreement FLOAT,  -- proporÃ§Ã£o que concordam
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela: PadrÃµes SensÃ­veis (dicionÃ¡rio)
CREATE TABLE sensitive_patterns (
    pattern_id SERIAL PRIMARY KEY,
    category VARCHAR(50) NOT NULL,  -- algoritmo, parametro, dataset, contato, metrica
    pattern_text VARCHAR(255) NOT NULL,
    is_regex BOOLEAN DEFAULT FALSE,
    weight FLOAT,  -- 0.0 a 1.0
    examples TEXT[],  -- exemplos de uso
    protection_suggestions TEXT[],  -- sugestÃµes de reescrita genÃ©rica
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela: Memoriais Protegidos (output)
CREATE TABLE memorials_protected (
    protected_id SERIAL PRIMARY KEY,
    memorial_id INT REFERENCES memorials(memorial_id),
    protected_text TEXT NOT NULL,
    sensitivity_level VARCHAR(20),  -- low, medium, high
    risk_score_original FLOAT,
    risk_score_protected FLOAT,
    clarity_original FLOAT,
    clarity_protected FLOAT,
    num_paragraphs_modified INT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Tabela: MÃ©tricas de ValidaÃ§Ã£o
CREATE TABLE validation_metrics (
    metric_id SERIAL PRIMARY KEY,
    corpus_size_memoriais INT,
    corpus_size_paragraphs INT,
    model_auc FLOAT,
    model_precision FLOAT,
    model_recall FLOAT,
    model_f1 FLOAT,
    inter_rater_kappa FLOAT,
    evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Ãndices para performance
CREATE INDEX idx_memorials_edital ON memorials(edital_id);
CREATE INDEX idx_memorials_sector ON memorials(sector);
CREATE INDEX idx_paragraphs_memorial ON paragraphs_annotated(memorial_id);
CREATE INDEX idx_paragraphs_has_exposure ON paragraphs_annotated(has_exposure);
CREATE INDEX idx_patterns_category ON sensitive_patterns(category);

-- Views Ãºteis
CREATE VIEW v_memorial_stats AS
SELECT 
    m.memorial_id,
    m.sector,
    m.technology_type,
    COUNT(p.paragraph_id) as num_paragraphs,
    SUM(CASE WHEN p.has_exposure THEN 1 ELSE 0 END) as exposure_count,
    AVG(p.entropy_normalized) as avg_entropy,
    AVG(p.num_sensitive_patterns) as avg_patterns
FROM memorials m
LEFT JOIN paragraphs_annotated p ON m.memorial_id = p.memorial_id
GROUP BY m.memorial_id, m.sector, m.technology_type;

CREATE VIEW v_sector_analysis AS
SELECT 
    sector,
    COUNT(DISTINCT memorial_id) as num_projects,
    AVG(CASE WHEN result = 'approved' THEN 1 ELSE 0 END) as approval_rate,
    AVG((SELECT AVG(entropy_normalized) FROM paragraphs_annotated pa 
         WHERE pa.memorial_id = memorials.memorial_id)) as avg_entropy_sector
FROM memorials
GROUP BY sector;
```

---

## 2. Gerador de Dados SintÃ©ticos (Python)

### Arquivo: `database/generate_synthetic_data.py`

```python
import random
import json
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import hashlib

class SyntheticDataGenerator:
    """Gera dados sintÃ©ticos para treinamento do EditalShield"""
    
    def __init__(self, seed: int = 42):
        random.seed(seed)
        self.seed = seed
        
    # ========== DADOS DE REFERÃŠNCIA ==========
    
    SECTORS = [
        "software", "hardware", "saas", "iot", "ai-ml", "biotech",
        "agritech", "fintech", "healthtech", "edtech", "ecommerce",
        "varejo", "logistics", "energia", "construtech"
    ]
    
    STAGES = ["pre-seed", "seed", "serie-a", "growth"]
    
    SECTION_TYPES = ["technical", "market", "team", "admin", "budget"]
    
    TECH_TERMS = {
        "software": ["API", "microservices", "pipeline", "engine", "framework", "SDK"],
        "ai-ml": ["modelo", "rede neural", "CNN", "RNN", "transformer", "embedding"],
        "iot": ["sensor", "gateway", "protocolo", "mqtt", "edge computing"],
        "hardware": ["PCB", "FPGA", "microcontroller", "chipset"],
        "blockchain": ["smart contract", "ledger", "consensus", "wallet"],
    }
    
    COMPANY_NAMES = [
        "TechVision", "DataFlow", "InnovateLabs", "CloudCore",
        "NeuralNet", "QuantumLeap", "SwiftAI", "PureData",
        "SecureVault", "BioSync", "GreenTech", "SmartCity"
    ]
    
    EDITALS_METADATA = [
        {
            "name": "Centelha Bahia III",
            "agency": "SECTI-BA",
            "min_value": 80000,
            "max_value": 120000,
            "months": 12,
            "approval_rate": 0.38,
            "sectors": ["software", "hardware", "saas"],
            "stages": ["pre-seed", "seed"],
            "detail_level": "medium"
        },
        {
            "name": "PIPE FAPESP Fase 1",
            "agency": "FAPESP",
            "min_value": 100000,
            "max_value": 200000,
            "months": 12,
            "approval_rate": 0.25,
            "sectors": ["software", "ai-ml", "biotech"],
            "stages": ["seed", "serie-a"],
            "detail_level": "high"
        },
        {
            "name": "Finep InovaÃ§Ã£o Conecta",
            "agency": "FINEP",
            "min_value": 50000,
            "max_value": 500000,
            "months": 24,
            "approval_rate": 0.35,
            "sectors": ["hardware", "agritech", "energia"],
            "stages": ["seed", "serie-a"],
            "detail_level": "medium"
        },
        {
            "name": "CNPq RHAE",
            "agency": "CNPq",
            "min_value": 30000,
            "max_value": 150000,
            "months": 12,
            "approval_rate": 0.42,
            "sectors": ["software", "ai-ml"],
            "stages": ["pre-seed", "seed"],
            "detail_level": "low"
        },
        {
            "name": "SEBRAE ALI",
            "agency": "SEBRAE",
            "min_value": 5000,
            "max_value": 20000,
            "months": 6,
            "approval_rate": 0.60,
            "sectors": ["varejo", "ecommerce", "logistics"],
            "stages": ["pre-seed"],
            "detail_level": "low"
        }
    ]
    
    # ========== GERADOR DE MEMORIAIS ==========
    
    def generate_memorial_paragraph(self, sector: str, section_type: str, 
                                   has_exposure: bool = False) -> Tuple[str, Dict]:
        """Gera um parÃ¡grafo de memorial tÃ©cnico com ou sem exposiÃ§Ã£o de PI"""
        
        company = random.choice(self.COMPANY_NAMES)
        
        # Templates by section
        if section_type == "technical":
            templates = [
                f"Nossa soluÃ§Ã£o {company} utiliza {random.choice(self.TECH_TERMS.get(sector, ['tÃ©cnicas avanÃ§adas']))} para otimizar processos de {sector}.",
                f"Desenvolvemos {random.choice(['algoritmo', 'modelo', 'framework'])} proprietÃ¡rio para anÃ¡lise de dados em tempo real.",
                f"O sistema integra {{tech1}} com {{tech2}} para criar pipeline de processamento eficiente.",
                f"Utilizamos {{method}} para validar a soluÃ§Ã£o em ambiente {{env}}.",
            ]
            
            if has_exposure:
                # Adiciona informaÃ§Ã£o sensÃ­vel
                sensitive_additions = [
                    f" O algoritmo BehaviorAnalyzer V2 aplica parÃ¢metros W=0.7, K=1.5.",
                    f" Dataset privado de 2M transaÃ§Ãµes com acurÃ¡cia 94.2%.",
                    f" Pipeline Lambda com processos parametrizados (threshold=0.8, decay=0.95).",
                    f" Contato estratÃ©gico: Dr. JoÃ£o Silva (j.silva@competitor.com).",
                    f" ROI validado em 3 clientes: CinemaChain, RetailCorp, FinTechBR.",
                ]
                text = random.choice(templates)
                text += random.choice(sensitive_additions)
            else:
                text = random.choice(templates)
                text = text.format(
                    tech1=random.choice(self.TECH_TERMS.get(sector, ["tÃ©cnicas"])),
                    tech2=random.choice(self.TECH_TERMS.get(sector, ["metodologias"])),
                    method=random.choice(["validaÃ§Ã£o A/B", "prototipagem", "teste piloto"]),
                    env=random.choice(["desenvolvimento", "staging", "produÃ§Ã£o"])
                )
                
        elif section_type == "market":
            if has_exposure:
                text = f"TAM estimado em R$ 500M. Clientes potenciais incluem {', '.join(random.sample(self.COMPANY_NAMES, 2))}. CAC: R$ 2500, LTV: R$ 85000 (34x)."
            else:
                text = f"Mercado de {sector} em expansÃ£o de 25-30% a.a. Nosso TAM Ã© conservador. Modelo de receita B2B SaaS com preÃ§o estratÃ©gico."
                
        elif section_type == "team":
            text = f"Equipe com {random.randint(3, 8)} pessoas. CTO: formado em {random.choice(['Stanford', 'MIT', 'USP', 'ITA'])}. CEO com 10+ anos de experiÃªncia em startups."
            
        elif section_type == "admin":
            text = "Cronograma realista com milestones claros. OrÃ§amento alocado conforme planejamento tÃ©cnico. Rigor em gestÃ£o de riscos."
            
        else:
            text = "SeÃ§Ã£o padrÃ£o do memorial com informaÃ§Ãµes gerais sobre o projeto."
        
        # Calcula entropia aproximada
        words = text.split()
        unique_words = len(set([w.lower() for w in words]))
        entropy = min(len(text) / 100, 10.0)  # HeurÃ­stica simples
        
        return text, {
            "num_words": len(words),
            "unique_words": unique_words,
            "entropy_approx": entropy,
            "has_exposure": has_exposure
        }
    
    def generate_memorial(self, sector: str = None, stage: str = None,
                         num_paragraphs: int = 18) -> Dict:
        """Gera um memorial completo (18-20 parÃ¡grafos tÃ­picos)"""
        
        sector = sector or random.choice(self.SECTORS)
        stage = stage or random.choice(self.STAGES)
        
        memorial = {
            "sector": sector,
            "stage": stage,
            "technology_type": sector,
            "paragraphs": [],
            "total_words": 0
        }
        
        # Estrutura tÃ­pica de memorial
        section_sequence = [
            "admin",  # intro
            "market", "market",  # contexto
            "technical", "technical", "technical",  # tech detail (maior chance de exposiÃ§Ã£o)
            "technical", "technical", "technical",
            "team", "team",  # equipe
            "admin", "admin", "admin",  # orÃ§amento, cronograma
            "market",  # projeÃ§Ãµes
            "admin", "admin"  # misc
        ]
        
        for i, section in enumerate(section_sequence[:num_paragraphs]):
            # 15-20% de chance de exposiÃ§Ã£o de PI em seÃ§Ãµes tÃ©cnicas
            has_exposure = (section == "technical" and random.random() < 0.18)
            
            text, stats = self.generate_memorial_paragraph(sector, section, has_exposure)
            
            memorial["paragraphs"].append({
                "index": i,
                "section": section,
                "text": text,
                "stats": stats,
                "has_exposure": has_exposure
            })
            
            memorial["total_words"] += stats["num_words"]
        
        return memorial
    
    # ========== GERADOR DE EDITAIS ==========
    
    def generate_edital(self) -> Dict:
        """Gera metadados de um edital"""
        base = random.choice(self.EDITALS_METADATA)
        
        # VariaÃ§Ã£o ligeira nos valores
        min_val = base["min_value"] + random.randint(-10000, 10000)
        max_val = base["max_value"] + random.randint(-10000, 10000)
        
        return {
            "name": base["name"],
            "agency": base["agency"],
            "min_value": max(min_val, 20000),
            "max_value": max(max_val, min_val + 50000),
            "execution_months": base["months"],
            "approval_rate": base["approval_rate"],
            "eligible_sectors": base["sectors"],
            "eligible_stages": base["stages"],
            "technical_detail_level": base["detail_level"],
            "evaluation_type": random.choice(["comite_publico", "banca_fechada"]),
            "criteria": {
                "innovation": random.randint(20, 40),
                "viability": random.randint(20, 35),
                "impact": random.randint(15, 30),
                "team": random.randint(10, 25),
                "market": random.randint(5, 20)
            }
        }
    
    # ========== EXPORTAÃ‡ÃƒO ==========
    
    def generate_dataset(self, num_memorials: int = 50, 
                        num_editals: int = 80) -> Dict:
        """Gera dataset completo sintÃ©tico"""
        
        print(f"[*] Gerando {num_editals} editais...")
        editals = [self.generate_edital() for _ in range(num_editals)]
        
        print(f"[*] Gerando {num_memorials} memoriais...")
        memorials = []
        for i in range(num_memorials):
            if i % 10 == 0:
                print(f"    â†’ {i}/{num_memorials}")
            memorial = self.generate_memorial()
            memorial["id"] = i + 1
            memorial["edital_id"] = random.randint(1, num_editals)
            memorial["result"] = random.choice(["approved", "rejected", "under_review"],
                                              [0.4, 0.35, 0.25])
            memorials.append(memorial)
        
        return {
            "editals": editals,
            "memorials": memorials,
            "generated_at": datetime.now().isoformat(),
            "seed": self.seed
        }
    
    def to_json(self, dataset: Dict, filepath: str):
        """Salva dataset em JSON"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)
        print(f"[âœ“] Dataset salvo em: {filepath}")
    
    def to_sql_inserts(self, dataset: Dict, filepath: str):
        """Gera SQL INSERTs para popular banco"""
        with open(filepath, 'w', encoding='utf-8') as f:
            # Editais
            f.write("-- Inserting Editals\n")
            for edital in dataset["editals"]:
                sectors = "ARRAY[" + ",".join([f"'{s}'" for s in edital["eligible_sectors"]]) + "]"
                stages = "ARRAY[" + ",".join([f"'{s}'" for s in edital["eligible_stages"]]) + "]"
                criteria = json.dumps(edital["criteria"]).replace("'", "''")
                
                f.write(f"""
INSERT INTO editals (name, agency, min_value, max_value, execution_months, 
                     approval_rate_historical, eligible_sectors, eligible_stages,
                     technical_detail_level, evaluation_type, criteria_json)
VALUES ('{edital["name"]}', '{edital["agency"]}', {edital["min_value"]}, 
        {edital["max_value"]}, {edital["execution_months"]}, 
        {edital["approval_rate"]}, {sectors}, {stages},
        '{edital["technical_detail_level"]}', '{edital["evaluation_type"]}',
        '{criteria}'::jsonb);
""")
            
            # Memoriais
            f.write("\n-- Inserting Memorials\n")
            for memorial in dataset["memorials"]:
                text_escaped = memorial["paragraphs"][0]["text"].replace("'", "''")
                
                f.write(f"""
INSERT INTO memorials (edital_id, sector, technology_type, stage, result,
                       original_text, num_words, num_paragraphs, is_synthetic)
VALUES ({memorial['edital_id']}, '{memorial['sector']}', '{memorial['technology_type']}',
        '{memorial['stage']}', '{memorial['result']}', '{text_escaped}',
        {memorial['total_words']}, {len(memorial['paragraphs'])}, TRUE);
""")
        
        print(f"[âœ“] SQL INSERTs gerados em: {filepath}")


# ========== SCRIPT PRINCIPAL ==========

if __name__ == "__main__":
    generator = SyntheticDataGenerator(seed=42)
    
    # Gera dataset
    dataset = generator.generate_dataset(num_memorials=50, num_editals=80)
    
    # Salva em formatos Ãºteis
    generator.to_json(dataset, "data/synthetic_dataset.json")
    generator.to_sql_inserts(dataset, "data/synthetic_inserts.sql")
    
    print("\n[âœ“] Dataset sintÃ©tico completo!")
    print(f"    - {len(dataset['editals'])} editais")
    print(f"    - {len(dataset['memorials'])} memoriais")
    print(f"    - Total: ~{sum([m['total_words'] for m in dataset['memorials']])} palavras")
```

---

## 3. Script de PopulaÃ§Ã£o do BD

### Arquivo: `database/populate.sh`

```bash
#!/bin/bash

# ============================================================================
# Script para popular EditalShield BD com dados sintÃ©ticos
# ============================================================================

set -e

echo "[*] EditalShield Database Population"
echo "======================================"

# ConfiguraÃ§Ã£o
DB_NAME="editalshield_dev"
DB_USER="postgres"
DB_HOST="localhost"
DB_PORT="5432"

echo "[1/3] Criando banco de dados..."
psql -h $DB_HOST -U $DB_USER -p $DB_PORT -c "DROP DATABASE IF EXISTS $DB_NAME;" 2>/dev/null || true
psql -h $DB_HOST -U $DB_USER -p $DB_PORT -c "CREATE DATABASE $DB_NAME;"

echo "[2/3] Criando schema..."
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -p $DB_PORT -f database/schema.sql

echo "[3/3] Inserindo dados sintÃ©ticos..."
python database/generate_synthetic_data.py
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -p $DB_PORT -f data/synthetic_inserts.sql

echo ""
echo "[âœ“] Banco populado com sucesso!"
echo "    Conecte com: psql -h $DB_HOST -U $DB_USER -d $DB_NAME"
```

---

## 4. Conectar do Python (ORM)

### Arquivo: `database/models.py`

```python
from sqlalchemy import create_engine, Column, Integer, String, Float, Boolean, Text, DateTime, ARRAY, JSON
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from datetime import datetime

DATABASE_URL = "postgresql://postgres:password@localhost:5432/editalshield_dev"

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

Base = declarative_base()


class Edital(Base):
    __tablename__ = "editals"
    
    edital_id = Column(Integer, primary_key=True)
    name = Column(String, nullable=False)
    agency = Column(String)
    min_value = Column(Float)
    max_value = Column(Float)
    execution_months = Column(Integer)
    approval_rate_historical = Column(Float)
    eligible_sectors = Column(ARRAY(String))
    technical_detail_level = Column(String)
    criteria_json = Column(JSON)


class Memorial(Base):
    __tablename__ = "memorials"
    
    memorial_id = Column(Integer, primary_key=True)
    edital_id = Column(Integer)
    sector = Column(String)
    technology_type = Column(String)
    stage = Column(String)
    result = Column(String)
    original_text = Column(Text)
    num_words = Column(Integer)
    num_paragraphs = Column(Integer)
    is_synthetic = Column(Boolean, default=False)


class ParagraphAnnotated(Base):
    __tablename__ = "paragraphs_annotated"
    
    paragraph_id = Column(Integer, primary_key=True)
    memorial_id = Column(Integer)
    paragraph_index = Column(Integer)
    original_text = Column(Text)
    section_type = Column(String)
    has_exposure = Column(Boolean)
    entropy_value = Column(Float)
    entropy_normalized = Column(Float)
    num_sensitive_patterns = Column(Integer)


# Uso
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def load_training_data():
    db = SessionLocal()
    paragraphs = db.query(ParagraphAnnotated).all()
    return paragraphs
```

---

## 5. Arquivo de ConfiguraÃ§Ã£o

### Arquivo: `.env.example`

```bash
# Database
DB_NAME=editalshield_dev
DB_USER=postgres
DB_PASSWORD=password
DB_HOST=localhost
DB_PORT=5432

# Synthetic Data
SYNTHETIC_MEMORIALS=50
SYNTHETIC_EDITALS=80
RANDOM_SEED=42

# Model Training
MIN_EXPOSURE_PERCENTAGE=0.15
MAX_EXPOSURE_PERCENTAGE=0.25
TRAIN_TEST_SPLIT=0.8

# Paths
DATA_PATH=./data
DATABASE_PATH=./database
MODELS_PATH=./models
```

---

## 6. Makefile para Facilitar

### Arquivo: `Makefile`

```makefile
.PHONY: help setup populate clean test

help:
	@echo "EditalShield Database Management"
	@echo "=================================="
	@echo "make setup       - Setup database and install deps"
	@echo "make populate    - Generate synthetic data and populate BD"
	@echo "make test        - Test data loading"
	@echo "make clean       - Drop database and clean data"

setup:
	@echo "[*] Installing dependencies..."
	pip install sqlalchemy psycopg2-binary python-dotenv
	@echo "[*] Creating directories..."
	mkdir -p data database models

populate:
	@echo "[*] Generating synthetic data..."
	python database/generate_synthetic_data.py
	@echo "[*] Populating database..."
	bash database/populate.sh

test:
	@echo "[*] Testing data loading..."
	python -c "from database.models import SessionLocal, Memorial; db = SessionLocal(); count = db.query(Memorial).count(); print(f'âœ“ {count} memoriais carregados')"

clean:
	@echo "[*] Cleaning..."
	rm -rf data/*.json data/*.sql
	psql -U postgres -c "DROP DATABASE IF EXISTS editalshield_dev;" 2>/dev/null || true
	@echo "[âœ“] Limpo!"

full: setup populate test
	@echo "[âœ“] Setup completo!"
```

---

## 7. Resumo de Arquivos

```
editalshield/
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ schema.sql                    # â† Schema PostgreSQL
â”‚   â”œâ”€â”€ generate_synthetic_data.py    # â† Gerador de dados
â”‚   â”œâ”€â”€ models.py                     # â† ORM SQLAlchemy
â”‚   â””â”€â”€ populate.sh                   # â† Script shell
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic_dataset.json        # â† Dataset gerado
â”‚   â””â”€â”€ synthetic_inserts.sql         # â† SQL para BD
â”œâ”€â”€ .env.example                       # â† ConfiguraÃ§Ã£o
â””â”€â”€ Makefile                           # â† AutomaÃ§Ã£o
```

---

## 8. Como Usar

### Passo 1: Setup inicial
```bash
make setup
```

### Passo 2: Popular BD
```bash
make populate
```

### Passo 3: Verificar dados
```bash
python -c "
from database.models import SessionLocal, Memorial, ParagraphAnnotated
db = SessionLocal()
print(f'Memoriais: {db.query(Memorial).count()}')
print(f'ParÃ¡grafos: {db.query(ParagraphAnnotated).count()}')
"
```

### Passo 4: Usar em treinamento
```python
from database.models import SessionLocal, ParagraphAnnotated

db = SessionLocal()
paragraphs = db.query(ParagraphAnnotated).filter(
    ParagraphAnnotated.has_exposure == True
).all()

print(f"ParÃ¡grafos com exposiÃ§Ã£o: {len(paragraphs)}")

# X = [p.entropy_normalized for p in paragraphs]
# y = [p.has_exposure for p in paragraphs]
# Treinar modelo Bayes...
```

---

## âœ… O Que VocÃª Ganha

| Aspecto | Antes | Depois |
|---------|-------|--------|
| **Dados de treinamento** | Nenhum (gap crÃ­tico) | 50+ memoriais sintÃ©ticos |
| **ParÃ¡grafos anotados** | 0 | ~1000 com ground truth |
| **Editais de referÃªncia** | 5 (manual) | 80 gerados |
| **Setup do BD** | Manual, propenso a erro | AutomÃ¡tico com `make populate` |
| **Reprodutibilidade** | ImpossÃ­vel | Garantida (seed=42) |
| **Tempo para produÃ§Ã£o** | 3-4 semanas | 1 semana (gap resolvido!) |

---

## ğŸ¯ PrÃ³ximo Passo

Com isso pronto, vocÃª pode:

1. âœ… **Treinar modelo Bayesiano** com dados reais + sintÃ©ticos
2. âœ… **Validar AUC** em 5-fold CV
3. âœ… **Implementar MÃ³dulo 4** (Memorial Protector)
4. âœ… **Publicar whitepaper** com resultados replicÃ¡veis

Quer que eu gere agora:
- **Script de treinamento** (Naive Bayes com sklearn)?
- **Notebook de validaÃ§Ã£o** (AUC, ROC, confusion matrix)?
- **Dashboard Streamlit** (visualizar dados + mÃ©tricas)?

Qual Ã© o prÃ³ximo passo? ğŸš€
